# BdE: Detecci√≥n de Fraude en Transacciones

**Proyecto de ciencia de datos end-to-end** que implementa un sistema de detecci√≥n de fraude utilizando datos p√∫blicos de pagos con tarjeta.  

El caso pr√°ctico se basa en transacciones minoristas (dataset p√∫blico), pero la **metodolog√≠a es transferible a sistemas de pagos de gran valor como TARGET2 o SNCE**, donde el Banco de Espa√±a act√∫a como **operador y supervisor**.  

---

## üö® Contexto

El fraude y las anomal√≠as en transacciones financieras suponen un **riesgo para la confianza y resiliencia de los sistemas de pago**.  
Este proyecto explora una soluci√≥n que combina **modelos cl√°sicos de ML** con **embeddings secuenciales** para capturar patrones ocultos en el comportamiento de los usuarios.  

---

## üìä Dataset

- **Fuente:** FraudNLP (Boulieris et al., 2023)  
- **Volumen:** 105.302 transacciones (feb‚Äìoct 2020, banco europeo)  
- **Clases:**  
  - 99,9% leg√≠timas (105.201)  
  - 0,096% fraudulentas (101)  
- **Contenido:**  
  - Secuencias completas de acciones de usuario en la web (‚âà1.900 tipos)  
  - Variables adicionales: importe, tiempos entre acciones, dispositivo, IP, frecuencia del beneficiario, etiqueta fraude/no fraude  

---

## üß© Metodolog√≠a

### 1. An√°lisis Exploratorio (EDA)  
- Distribuci√≥n de importes y acciones  
- Se√±ales d√©biles de fraude ‚Üí necesidad de ir m√°s all√° de datos tabulares  

### 2. Ingenier√≠a de Features  
- Variables cl√°sicas (importe, dispositivo, IP, etc.)  
- Estad√≠sticos temporales (tiempo medio entre acciones, desviaci√≥n t√≠pica, tiempo a la primera acci√≥n, total de acciones, etc.)  
- Embeddings secuenciales (128D) obtenidos con un **Transformer Encoder** que resume el historial de acciones de cada usuario  

### 3. Modelado  
- Algoritmo principal: **LightGBM**  
- Estrategias:  
  1. GB con features tabulares  
  2. GB + SMOTE  
  3. GB + Embeddings  
  4. GB + Embeddings + SMOTE  

- M√©tricas:  
  - **AU-PRC** (Average Precision-Recall Curve)  
  - **F1, F2, F0.5**  

## 4. Resultados

| M√©trica   | GB    | GB + SMOTE | Encoder + GB | Encoder + GB + SMOTE |
|-----------|-------|------------|--------------|-----------------------|
| **AU-PRC** | 0.3300 | 0.2700     | 0.4400       | **0.5310**            |
| **F1**     | 0.3200 | 0.2940     | **0.5420**   | 0.4140                |
| **F2**     | 0.3210 | 0.2450     | **0.5600**   | 0.5510                |
| **F0.5**   | **0.5560** | 0.0000     | 0.5440       | 0.4810                |

**Conclusiones principales:**
- Los **embeddings secuenciales** elevan el rendimiento de forma notable (mejoras en todas las m√©tricas frente al baseline).  
- El **uso aislado de SMOTE** degrada las m√©tricas ‚Üí solo es √∫til en combinaci√≥n con embeddings.  
- Seg√∫n la m√©trica priorizada:  
  - **Encoder + GB** es √≥ptimo en F1 y F2 (mejor balance y recall).  
  - **Encoder + GB + SMOTE** alcanza el mejor AU-PRC.  

---

## ‚öñÔ∏è Explicabilidad

- Los embeddings dificultan la interpretaci√≥n directa  
- Se utilizan **SHAP values** para analizar la contribuci√≥n de:  
  - Variables tabulares cl√°sicas  
  - Dimensiones de embeddings secuenciales (`z_seq_x`)  

---

## ‚öôÔ∏è Pipeline

- **Entrenamiento reproducible** con divisi√≥n estratificada (60/20/20)  
- **SMOTE** aplicado solo cuando es beneficioso  
- **Validaci√≥n cruzada** con *Repeated Stratified K-Fold*  
- **Tracking y versionado** con **MLflow**  
- **CI/CD** con **GitHub Actions**  

---


